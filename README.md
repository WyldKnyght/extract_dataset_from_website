Extracting data from websites and saving them as datasets

# Phase 1 - Web Crawler for Robots.txt

This project is a web crawler that reads a list of websites from a file, checks if each website allows web crawling by reading its robots.txt file, and then stores the robots.txt data in a SQLite database.

## Installation

1. Clone the repository to your local machine.
2. Ensure you have Python installed. This project requires Python 3.6 or higher.

## Usage

1. Add the websites you want to crawl to the `websites_to_scrape.txt` file. Each website URL should be on a new line.
2. Run `main.py` to start the web crawler.

## Contributing

Contributions are welcome. Please open an issue to discuss your ideas before making changes.

## License

This project is licensed under the terms of the MIT license.

## Contact

If you have any questions or want to collaborate, feel free to reach out.
